# Gamatrain AI Research ğŸ¤–

Fine-tuning a Large Language Model (Qwen2-1.5B) with Gamatrain's educational content to create an intelligent tutor assistant.

## ğŸ¯ Project Goal

Create an AI assistant that can:
- Answer questions about Gamatrain's educational content (courses, tests, blogs)
- Maintain general intelligence (math, logic, reasoning)
- Be deployed locally using Ollama

## ğŸ“Š Results

| Metric | Value |
|--------|-------|
| Base Model | Qwen2-1.5B-Instruct |
| Final Dataset | 2,614 samples |
| Domain Data | 2,422 (Gamatrain blogs, tests, courses) |
| General Data | 192 (math, logic, chat - weighted 4x) |
| Output Format | GGUF (4-bit quantized) |

## ğŸ—‚ï¸ Repository Structure

```
gamatrain-ai-research/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ gamatrain_final_dataset.jsonl  # Final training dataset
â”‚   â”œâ”€â”€ gamatrain_finetune_data.jsonl  # Raw Gamatrain data
â”‚   â”œâ”€â”€ general_knowledge.jsonl        # General knowledge samples
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ extract_and_format_data.py # API data extraction
â”‚       â”œâ”€â”€ extract_blog_data.py       # Blog sitemap extraction
â”‚       â”œâ”€â”€ generate_general_data.py   # General knowledge generator
â”‚       â”œâ”€â”€ create_final_dataset.py    # Dataset merger
â”‚       â””â”€â”€ requirements.txt
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ Modelfile              # Ollama model configuration
â”‚   â””â”€â”€ README.md              # Model download instructions
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ llm_server.py          # FastAPI server
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ fine-tuning-demo.ipynb # Google Colab training notebook
â””â”€â”€ docs/
    â”œâ”€â”€ RESEARCH.md            # Research findings
    â”œâ”€â”€ TRAINING.md            # Training guide
    â””â”€â”€ DEPLOYMENT.md          # Deployment guide
```

## ğŸš€ Quick Start

### 1. Download the Model
See [model/README.md](model/README.md) for download instructions.

### 2. Import to Ollama
```bash
cd model/
ollama create gamatrain-qwen -f Modelfile
```

### 3. Test the Model
```bash
ollama run gamatrain-qwen "What is 2 + 2?"
ollama run gamatrain-qwen "Tell me about Ohm's Law"
ollama run gamatrain-qwen "Does Gamatrain have past papers for Biology?"
```

### 4. Run the API Server (Optional)
```bash
cd api/
pip install -r requirements.txt
python llm_server.py
```

## ğŸ“š Documentation

- **[RESEARCH.md](docs/RESEARCH.md)** - Problem statement, approach, and findings
- **[TRAINING.md](docs/TRAINING.md)** - How to fine-tune the model
- **[DEPLOYMENT.md](docs/DEPLOYMENT.md)** - How to deploy with Ollama

## âš ï¸ Key Learning: Catastrophic Forgetting

During development, we discovered that fine-tuning only on domain-specific data caused the model to "forget" basic abilities (like math). The solution was to mix domain data with general knowledge samples.

**Before fix:** `2 + 2 = 0` âŒ
**After fix:** `2 + 2 = 4` âœ…

## ğŸ“„ License

MIT License

## ğŸ¤ Contributing

Contributions are welcome! Please read the documentation first.
